{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f05d7a9b",
   "metadata": {},
   "source": [
    "# Marine Ecosystem Data Ingestion Pipeline\n",
    "\n",
    "This notebook establishes the data ingestion pipeline for the Marine Ecosystem Health & Species Presence Prediction Platform. It demonstrates how to access and ingest data from multiple public sources including satellite imagery, ocean physics data, in-situ sensors, species observations, and bathymetry data.\n",
    "\n",
    "## üåä SDG 14: Life Below Water Data Sources\n",
    "\n",
    "- **Satellite Imagery**: Sentinel-2/3, MODIS chlorophyll & turbidity\n",
    "- **Ocean Physics**: NOAA OSTIA SST, CMEMS currents/salinity\n",
    "- **In-situ Sensors**: Argo floats, NOAA Buoy network\n",
    "- **Species Data**: OBIS API, iNaturalist exports\n",
    "- **Bathymetry**: SRTM30 + GEBCO elevation data\n",
    "\n",
    "## Target Region: Pacific Ocean (Example: 150¬∞W-120¬∞W, 20¬∞N-50¬∞N)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51432389",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Required Libraries\n",
    "import os\n",
    "import sys\n",
    "import json\n",
    "import requests\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime, timedelta\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Geospatial and Earth Observation\n",
    "try:\n",
    "    import ee\n",
    "    print(\"Earth Engine API imported successfully\")\n",
    "except ImportError:\n",
    "    print(\"Earth Engine API not installed. Run: pip install earthengine-api\")\n",
    "\n",
    "import rasterio\n",
    "import geopandas as gpd\n",
    "import xarray as xr\n",
    "from shapely.geometry import Point, Polygon\n",
    "import folium\n",
    "\n",
    "# Cloud Storage\n",
    "try:\n",
    "    import boto3\n",
    "    print(\"AWS SDK imported successfully\")\n",
    "except ImportError:\n",
    "    print(\"AWS SDK not installed. Run: pip install boto3\")\n",
    "\n",
    "try:\n",
    "    from google.cloud import storage\n",
    "    print(\"Google Cloud Storage imported successfully\")\n",
    "except ImportError:\n",
    "    print(\"Google Cloud SDK not installed. Run: pip install google-cloud-storage\")\n",
    "\n",
    "# Configuration\n",
    "pd.set_option('display.max_columns', None)\n",
    "plt.style.use('seaborn-v0_8')\n",
    "print(\"All libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f53f68da",
   "metadata": {},
   "source": [
    "## 1. Configure Data Source Access\n",
    "\n",
    "Set up authentication and API keys for accessing public datasets. You'll need to configure:\n",
    "\n",
    "1. **Google Earth Engine**: Sign up at https://earthengine.google.com/\n",
    "2. **AWS Account**: For public datasets access\n",
    "3. **NOAA API**: Register for API keys at https://www.ncdc.noaa.gov/cdo-web/webservices/v2\n",
    "4. **OBIS API**: Ocean Biogeographic Information System (open access)\n",
    "5. **iNaturalist**: Public data exports (no API key needed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac09e02b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration for Data Source Access\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# Create data directories\n",
    "data_dir = Path(\"../data\")\n",
    "raw_data_dir = data_dir / \"raw\"\n",
    "processed_data_dir = data_dir / \"processed\"\n",
    "\n",
    "for directory in [raw_data_dir, processed_data_dir]:\n",
    "    directory.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Target region configuration (Pacific Ocean example)\n",
    "TARGET_REGION = {\n",
    "    'west': -150.0,   # 150¬∞W\n",
    "    'east': -120.0,   # 120¬∞W\n",
    "    'south': 20.0,    # 20¬∞N\n",
    "    'north': 50.0     # 50¬∞N\n",
    "}\n",
    "\n",
    "# Time range for data collection\n",
    "START_DATE = '2023-01-01'\n",
    "END_DATE = '2023-12-31'\n",
    "\n",
    "# Data source configurations\n",
    "NOAA_API_KEY = os.getenv('NOAA_API_KEY', 'your_noaa_api_key_here')\n",
    "AWS_ACCESS_KEY = os.getenv('AWS_ACCESS_KEY_ID', 'your_aws_access_key')\n",
    "AWS_SECRET_KEY = os.getenv('AWS_SECRET_ACCESS_KEY', 'your_aws_secret_key')\n",
    "\n",
    "# Earth Engine Authentication (run ee.Authenticate() first time)\n",
    "try:\n",
    "    ee.Initialize()\n",
    "    print(\"Earth Engine initialized successfully\")\n",
    "except Exception as e:\n",
    "    print(f\"Earth Engine initialization failed: {e}\")\n",
    "    print(\"Run 'ee.Authenticate()' to set up authentication\")\n",
    "\n",
    "print(f\"Target Region: {TARGET_REGION}\")\n",
    "print(f\"Date Range: {START_DATE} to {END_DATE}\")\n",
    "print(f\"Data directories created: {raw_data_dir}, {processed_data_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d49f767c",
   "metadata": {},
   "source": [
    "## 2. Ingest Satellite Imagery Data\n",
    "\n",
    "Download Sentinel-2/3 and MODIS data using Earth Engine Python SDK. We'll focus on:\n",
    "\n",
    "- **Sentinel-2 MSI**: 10m resolution for coastal areas\n",
    "- **Sentinel-3 OLCI**: 300m resolution for open ocean\n",
    "- **MODIS**: Chlorophyll-a concentration and sea surface temperature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73e5a8ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ingest_satellite_imagery():\n",
    "    \"\"\"\n",
    "    Ingest satellite imagery from Google Earth Engine\n",
    "    \"\"\"\n",
    "    satellite_data = {}\n",
    "    \n",
    "    try:\n",
    "        # Define the area of interest\n",
    "        aoi = ee.Geometry.Rectangle([\n",
    "            TARGET_REGION['west'], TARGET_REGION['south'],\n",
    "            TARGET_REGION['east'], TARGET_REGION['north']\n",
    "        ])\n",
    "        \n",
    "        # MODIS Ocean Color Data (Chlorophyll-a)\n",
    "        print(\"Fetching MODIS Ocean Color data...\")\n",
    "        modis_collection = ee.ImageCollection('NASA/OCEANDATA/MODIS-Aqua/L3SMI') \\\n",
    "            .filterDate(START_DATE, END_DATE) \\\n",
    "            .filterBounds(aoi) \\\n",
    "            .select(['chlor_a'])\n",
    "        \n",
    "        # Get the latest image\n",
    "        modis_latest = modis_collection.sort('system:time_start', False).first()\n",
    "        \n",
    "        # MODIS Sea Surface Temperature\n",
    "        print(\"Fetching MODIS SST data...\")\n",
    "        modis_sst_collection = ee.ImageCollection('NASA/OCEANDATA/MODIS-Aqua/L3SMI') \\\n",
    "            .filterDate(START_DATE, END_DATE) \\\n",
    "            .filterBounds(aoi) \\\n",
    "            .select(['sst'])\n",
    "        \n",
    "        modis_sst_latest = modis_sst_collection.sort('system:time_start', False).first()\n",
    "        \n",
    "        # Sentinel-3 OLCI (if available)\n",
    "        print(\"Checking Sentinel-3 OLCI availability...\")\n",
    "        try:\n",
    "            s3_collection = ee.ImageCollection('COPERNICUS/S3/OLCI') \\\n",
    "                .filterDate(START_DATE, END_DATE) \\\n",
    "                .filterBounds(aoi)\n",
    "            s3_latest = s3_collection.sort('system:time_start', False).first()\n",
    "            print(\"Sentinel-3 OLCI data found\")\n",
    "        except:\n",
    "            print(\"Sentinel-3 OLCI data not available in Earth Engine\")\n",
    "            s3_latest = None\n",
    "        \n",
    "        satellite_data = {\n",
    "            'modis_chlorophyll': modis_latest,\n",
    "            'modis_sst': modis_sst_latest,\n",
    "            'sentinel3_olci': s3_latest\n",
    "        }\n",
    "        \n",
    "        print(\"Satellite imagery data fetched successfully!\")\n",
    "        return satellite_data\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error fetching satellite data: {e}\")\n",
    "        return None\n",
    "\n",
    "# Fetch satellite imagery\n",
    "satellite_data = ingest_satellite_imagery()\n",
    "\n",
    "if satellite_data:\n",
    "    print(\"Available satellite datasets:\")\n",
    "    for dataset, data in satellite_data.items():\n",
    "        if data:\n",
    "            print(f\"  ‚úì {dataset}\")\n",
    "        else:\n",
    "            print(f\"  ‚úó {dataset} (not available)\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è Satellite data ingestion failed. Check Earth Engine authentication.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88a117b7",
   "metadata": {},
   "source": [
    "## 3. Ingest Ocean Physics Data\n",
    "\n",
    "Fetch sea surface temperature, currents, salinity, and sea level data from:\n",
    "\n",
    "- **NOAA OSTIA**: Sea Surface Temperature\n",
    "- **CMEMS**: Copernicus Marine Environment Monitoring Service\n",
    "- **HYCOM**: Ocean circulation models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cb37833",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ingest_ocean_physics_data():\n",
    "    \"\"\"\n",
    "    Ingest ocean physics data from various sources\n",
    "    \"\"\"\n",
    "    ocean_data = {}\n",
    "    \n",
    "    # NOAA OSTIA Sea Surface Temperature\n",
    "    print(\"Fetching NOAA OSTIA SST data...\")\n",
    "    try:\n",
    "        # NOAA OSTIA SST data via THREDDS server\n",
    "        ostia_url = \"https://www.ncei.noaa.gov/thredds/dodsC/OisstBase/NetCDF/V2.1/AVHRR/202301/oisst-avhrr-v02r01.20230101.nc\"\n",
    "        \n",
    "        # Alternative: Use xarray to access remote datasets\n",
    "        # This is a demo URL - you'd need to construct the proper URL for your date range\n",
    "        ostia_demo_url = \"https://psl.noaa.gov/thredds/dodsC/Datasets/noaa.oisst.v2/sst.mnmean.nc\"\n",
    "        \n",
    "        print(f\"Accessing: {ostia_demo_url}\")\n",
    "        sst_dataset = xr.open_dataset(ostia_demo_url)\n",
    "        \n",
    "        # Filter by region and time\n",
    "        sst_regional = sst_dataset.sel(\n",
    "            lat=slice(TARGET_REGION['south'], TARGET_REGION['north']),\n",
    "            lon=slice(TARGET_REGION['west'], TARGET_REGION['east']),\n",
    "            time=slice(START_DATE, END_DATE)\n",
    "        )\n",
    "        \n",
    "        ocean_data['sst_ostia'] = sst_regional\n",
    "        print(\"‚úì NOAA OSTIA SST data fetched\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚úó Error fetching OSTIA SST: {e}\")\n",
    "        ocean_data['sst_ostia'] = None\n",
    "    \n",
    "    # CMEMS Ocean Currents (demo - requires registration)\n",
    "    print(\"Setting up CMEMS data access...\")\n",
    "    try:\n",
    "        # This would require CMEMS credentials\n",
    "        # cmems_url = \"https://nrt.cmems-du.eu/thredds/dodsC/global-analysis-forecast-phy-001-024\"\n",
    "        \n",
    "        # For demo, we'll create synthetic data structure\n",
    "        print(\"‚ö†Ô∏è CMEMS requires registration at https://marine.copernicus.eu/\")\n",
    "        ocean_data['cmems_currents'] = None\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚úó Error with CMEMS: {e}\")\n",
    "        ocean_data['cmems_currents'] = None\n",
    "    \n",
    "    # Ocean salinity and other parameters\n",
    "    print(\"Accessing additional ocean physics data...\")\n",
    "    try:\n",
    "        # HYCOM Global Ocean Prediction System\n",
    "        hycom_url = \"https://tds.hycom.org/thredds/dodsC/GLBy0.08/expt_93.0\"\n",
    "        print(f\"Note: HYCOM data available at {hycom_url}\")\n",
    "        ocean_data['hycom'] = None  # Would implement full access with proper auth\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚úó Error with HYCOM: {e}\")\n",
    "        ocean_data['hycom'] = None\n",
    "    \n",
    "    return ocean_data\n",
    "\n",
    "# Fetch ocean physics data\n",
    "ocean_physics_data = ingest_ocean_physics_data()\n",
    "\n",
    "print(\"\\\\nOcean Physics Data Summary:\")\n",
    "for dataset, data in ocean_physics_data.items():\n",
    "    if data is not None:\n",
    "        print(f\"  ‚úì {dataset}: {type(data)}\")\n",
    "        if hasattr(data, 'dims'):\n",
    "            print(f\"    Dimensions: {data.dims}\")\n",
    "    else:\n",
    "        print(f\"  ‚úó {dataset}: Not available\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c41e9d5d",
   "metadata": {},
   "source": [
    "## 4. Ingest In-situ Sensor Data\n",
    "\n",
    "Stream or batch download sensor data from:\n",
    "\n",
    "- **Argo Floats**: Temperature and salinity profiles\n",
    "- **NOAA Buoy Network**: Wave height, wind, atmospheric pressure\n",
    "- **Tide Gauges**: Sea level measurements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f9ea06e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ingest_insitu_sensor_data():\n",
    "    \"\"\"\n",
    "    Ingest in-situ sensor data from Argo floats and NOAA buoys\n",
    "    \"\"\"\n",
    "    sensor_data = {}\n",
    "    \n",
    "    # Argo Float Data\n",
    "    print(\"Fetching Argo float data...\")\n",
    "    try:\n",
    "        # Argo data from Coriolis Data Portal\n",
    "        argo_url = \"https://data-argo.ifremer.fr/geo/pacific_ocean\"\n",
    "        \n",
    "        # For demo, we'll use the Global Argo Atlas API\n",
    "        # This provides access to Argo float profiles\n",
    "        print(\"Accessing Argo float profiles...\")\n",
    "        \n",
    "        # Example API call (would need proper implementation)\n",
    "        argo_api_url = f\"https://argovis.colorado.edu/selection/profiles\"\n",
    "        params = {\n",
    "            'startDate': START_DATE,\n",
    "            'endDate': END_DATE,\n",
    "            'shape': f\"[[[{TARGET_REGION['west']},{TARGET_REGION['south']}],\"\n",
    "                    f\"[{TARGET_REGION['east']},{TARGET_REGION['south']}],\"\n",
    "                    f\"[{TARGET_REGION['east']},{TARGET_REGION['north']}],\"\n",
    "                    f\"[{TARGET_REGION['west']},{TARGET_REGION['north']}],\"\n",
    "                    f\"[{TARGET_REGION['west']},{TARGET_REGION['south']}]]]\"\n",
    "        }\n",
    "        \n",
    "        # Make API request\n",
    "        response = requests.get(argo_api_url, params=params, timeout=30)\n",
    "        if response.status_code == 200:\n",
    "            argo_profiles = response.json()\n",
    "            sensor_data['argo_floats'] = argo_profiles[:100]  # Limit to first 100 profiles\n",
    "            print(f\"‚úì Fetched {len(sensor_data['argo_floats'])} Argo profiles\")\n",
    "        else:\n",
    "            print(f\"‚úó Argo API request failed: {response.status_code}\")\n",
    "            sensor_data['argo_floats'] = None\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"‚úó Error fetching Argo data: {e}\")\n",
    "        sensor_data['argo_floats'] = None\n",
    "    \n",
    "    # NOAA Buoy Data\n",
    "    print(\"Fetching NOAA buoy data...\")\n",
    "    try:\n",
    "        # NOAA NDBC (National Data Buoy Center) API\n",
    "        buoy_stations_url = \"https://www.ndbc.noaa.gov/activestations.xml\"\n",
    "        \n",
    "        # Get active stations in our region\n",
    "        buoy_response = requests.get(buoy_stations_url, timeout=30)\n",
    "        \n",
    "        if buoy_response.status_code == 200:\n",
    "            # Parse XML response (simplified)\n",
    "            print(\"‚úì NOAA buoy station list retrieved\")\n",
    "            \n",
    "            # For demo, let's use a specific buoy station\n",
    "            # Station 46001 - Gulf of Alaska\n",
    "            station_id = \"46001\"\n",
    "            buoy_data_url = f\"https://www.ndbc.noaa.gov/data/realtime2/{station_id}.txt\"\n",
    "            \n",
    "            buoy_data_response = requests.get(buoy_data_url, timeout=30)\n",
    "            if buoy_data_response.status_code == 200:\n",
    "                # Parse the data\n",
    "                buoy_lines = buoy_data_response.text.strip().split('\\\\n')\n",
    "                headers = buoy_lines[0].split()\n",
    "                units = buoy_lines[1].split()\n",
    "                \n",
    "                buoy_data_list = []\n",
    "                for line in buoy_lines[2:10]:  # First 8 data rows\n",
    "                    values = line.split()\n",
    "                    if len(values) == len(headers):\n",
    "                        buoy_data_list.append(dict(zip(headers, values)))\n",
    "                \n",
    "                sensor_data['noaa_buoys'] = {\n",
    "                    'station_id': station_id,\n",
    "                    'headers': headers,\n",
    "                    'units': units,\n",
    "                    'data': buoy_data_list\n",
    "                }\n",
    "                print(f\"‚úì Fetched NOAA buoy data for station {station_id}\")\n",
    "            else:\n",
    "                print(f\"‚úó Buoy data request failed: {buoy_data_response.status_code}\")\n",
    "                sensor_data['noaa_buoys'] = None\n",
    "        else:\n",
    "            print(f\"‚úó Buoy stations request failed: {buoy_response.status_code}\")\n",
    "            sensor_data['noaa_buoys'] = None\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"‚úó Error fetching buoy data: {e}\")\n",
    "        sensor_data['noaa_buoys'] = None\n",
    "    \n",
    "    return sensor_data\n",
    "\n",
    "# Fetch in-situ sensor data\n",
    "insitu_data = ingest_insitu_sensor_data()\n",
    "\n",
    "print(\"\\\\nIn-situ Sensor Data Summary:\")\n",
    "for dataset, data in insitu_data.items():\n",
    "    if data is not None:\n",
    "        if dataset == 'argo_floats':\n",
    "            print(f\"  ‚úì {dataset}: {len(data)} profiles\")\n",
    "        elif dataset == 'noaa_buoys':\n",
    "            print(f\"  ‚úì {dataset}: Station {data['station_id']}, {len(data['data'])} records\")\n",
    "        else:\n",
    "            print(f\"  ‚úì {dataset}: {type(data)}\")\n",
    "    else:\n",
    "        print(f\"  ‚úó {dataset}: Not available\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c18a8bfa",
   "metadata": {},
   "source": [
    "## 5. Ingest Species Observation Data\n",
    "\n",
    "Query marine species databases for biodiversity and occurrence data:\n",
    "\n",
    "- **OBIS**: Ocean Biogeographic Information System API\n",
    "- **iNaturalist**: Public biodiversity observations\n",
    "- **GBIF**: Global Biodiversity Information Facility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbc3295b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ingest_species_data():\n",
    "    \"\"\"\n",
    "    Ingest marine species observation data from OBIS and other sources\n",
    "    \"\"\"\n",
    "    species_data = {}\n",
    "    \n",
    "    # OBIS (Ocean Biogeographic Information System) API\n",
    "    print(\"Fetching OBIS species data...\")\n",
    "    try:\n",
    "        obis_base_url = \"https://api.obis.org/v3/occurrence\"\n",
    "        \n",
    "        # Parameters for our target region\n",
    "        obis_params = {\n",
    "            'geometry': f\"POLYGON(({TARGET_REGION['west']} {TARGET_REGION['south']},\"\n",
    "                       f\"{TARGET_REGION['east']} {TARGET_REGION['south']},\"\n",
    "                       f\"{TARGET_REGION['east']} {TARGET_REGION['north']},\"\n",
    "                       f\"{TARGET_REGION['west']} {TARGET_REGION['north']},\"\n",
    "                       f\"{TARGET_REGION['west']} {TARGET_REGION['south']}))\",\n",
    "            'startdate': START_DATE,\n",
    "            'enddate': END_DATE,\n",
    "            'size': 1000  # Limit to 1000 records for demo\n",
    "        }\n",
    "        \n",
    "        obis_response = requests.get(obis_base_url, params=obis_params, timeout=60)\n",
    "        \n",
    "        if obis_response.status_code == 200:\n",
    "            obis_data = obis_response.json()\n",
    "            species_data['obis_occurrences'] = obis_data.get('results', [])\n",
    "            print(f\"‚úì Fetched {len(species_data['obis_occurrences'])} OBIS occurrences\")\n",
    "            \n",
    "            # Get species summary\n",
    "            if species_data['obis_occurrences']:\n",
    "                species_names = [obs.get('species', 'Unknown') for obs in species_data['obis_occurrences']]\n",
    "                unique_species = list(set(species_names))\n",
    "                print(f\"  Found {len(unique_species)} unique species\")\n",
    "        else:\n",
    "            print(f\"‚úó OBIS API request failed: {obis_response.status_code}\")\n",
    "            species_data['obis_occurrences'] = None\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"‚úó Error fetching OBIS data: {e}\")\n",
    "        species_data['obis_occurrences'] = None\n",
    "    \n",
    "    # GBIF (Global Biodiversity Information Facility) API\n",
    "    print(\"Fetching GBIF marine species data...\")\n",
    "    try:\n",
    "        gbif_base_url = \"https://api.gbif.org/v1/occurrence/search\"\n",
    "        \n",
    "        # Parameters for marine species in our region\n",
    "        gbif_params = {\n",
    "            'decimalLatitude': f\"{TARGET_REGION['south']},{TARGET_REGION['north']}\",\n",
    "            'decimalLongitude': f\"{TARGET_REGION['west']},{TARGET_REGION['east']}\",\n",
    "            'basisOfRecord': 'OBSERVATION',\n",
    "            'hasCoordinate': 'true',\n",
    "            'hasGeospatialIssue': 'false',\n",
    "            'limit': 300  # Limit for demo\n",
    "        }\n",
    "        \n",
    "        gbif_response = requests.get(gbif_base_url, params=gbif_params, timeout=60)\n",
    "        \n",
    "        if gbif_response.status_code == 200:\n",
    "            gbif_data = gbif_response.json()\n",
    "            species_data['gbif_occurrences'] = gbif_data.get('results', [])\n",
    "            print(f\"‚úì Fetched {len(species_data['gbif_occurrences'])} GBIF occurrences\")\n",
    "        else:\n",
    "            print(f\"‚úó GBIF API request failed: {gbif_response.status_code}\")\n",
    "            species_data['gbif_occurrences'] = None\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"‚úó Error fetching GBIF data: {e}\")\n",
    "        species_data['gbif_occurrences'] = None\n",
    "    \n",
    "    # iNaturalist Research Grade Observations (marine)\n",
    "    print(\"Fetching iNaturalist marine observations...\")\n",
    "    try:\n",
    "        inaturalist_url = \"https://api.inaturalist.org/v1/observations\"\n",
    "        \n",
    "        # Parameters for marine observations\n",
    "        inat_params = {\n",
    "            'nelat': TARGET_REGION['north'],\n",
    "            'nelng': TARGET_REGION['east'],\n",
    "            'swlat': TARGET_REGION['south'],\n",
    "            'swlng': TARGET_REGION['west'],\n",
    "            'quality_grade': 'research',\n",
    "            'iconic_taxa': 'Actinopterygii,Mollusca,Cnidaria',  # Fish, Mollusks, Corals\n",
    "            'per_page': 200,\n",
    "            'order': 'desc',\n",
    "            'order_by': 'created_at'\n",
    "        }\n",
    "        \n",
    "        inat_response = requests.get(inaturalist_url, params=inat_params, timeout=60)\n",
    "        \n",
    "        if inat_response.status_code == 200:\n",
    "            inat_data = inat_response.json()\n",
    "            species_data['inaturalist_observations'] = inat_data.get('results', [])\n",
    "            print(f\"‚úì Fetched {len(species_data['inaturalist_observations'])} iNaturalist observations\")\n",
    "        else:\n",
    "            print(f\"‚úó iNaturalist API request failed: {inat_response.status_code}\")\n",
    "            species_data['inaturalist_observations'] = None\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"‚úó Error fetching iNaturalist data: {e}\")\n",
    "        species_data['inaturalist_observations'] = None\n",
    "    \n",
    "    return species_data\n",
    "\n",
    "# Fetch species observation data\n",
    "species_obs_data = ingest_species_data()\n",
    "\n",
    "print(\"\\\\nSpecies Observation Data Summary:\")\n",
    "for dataset, data in species_obs_data.items():\n",
    "    if data is not None:\n",
    "        print(f\"  ‚úì {dataset}: {len(data)} observations\")\n",
    "        \n",
    "        # Show sample species if available\n",
    "        if data and len(data) > 0:\n",
    "            if dataset == 'obis_occurrences':\n",
    "                sample_species = [obs.get('species', 'Unknown') for obs in data[:5]]\n",
    "            elif dataset == 'gbif_occurrences':\n",
    "                sample_species = [obs.get('species', obs.get('scientificName', 'Unknown')) for obs in data[:5]]\n",
    "            elif dataset == 'inaturalist_observations':\n",
    "                sample_species = [obs.get('taxon', {}).get('name', 'Unknown') for obs in data[:5]]\n",
    "            else:\n",
    "                sample_species = []\n",
    "            \n",
    "            if sample_species:\n",
    "                print(f\"    Sample species: {', '.join(set(sample_species))}\")\n",
    "    else:\n",
    "        print(f\"  ‚úó {dataset}: Not available\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78f6e65d",
   "metadata": {},
   "source": [
    "## 6. Ingest Bathymetry & Terrain Data\n",
    "\n",
    "Download elevation and bathymetry data for understanding seafloor topography:\n",
    "\n",
    "- **GEBCO**: General Bathymetric Chart of the Oceans\n",
    "- **SRTM30**: Shuttle Radar Topography Mission 30m\n",
    "- **ETOPO**: Earth Topography and Ocean Bathymetry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b26f7c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ingest_bathymetry_data():\n",
    "    \"\"\"\n",
    "    Ingest bathymetry and terrain data\n",
    "    \"\"\"\n",
    "    bathymetry_data = {}\n",
    "    \n",
    "    # GEBCO Bathymetry Data\n",
    "    print(\"Setting up GEBCO bathymetry data access...\")\n",
    "    try:\n",
    "        # GEBCO provides global bathymetric data\n",
    "        # The 2023 grid is available for download\n",
    "        gebco_info_url = \"https://www.gebco.net/data_and_products/gridded_bathymetry_data/\"\n",
    "        \n",
    "        # For demonstration, we'll use ETOPO data which is more accessible\n",
    "        print(\"Accessing ETOPO global relief data...\")\n",
    "        \n",
    "        # NOAA ETOPO Global Relief Model\n",
    "        etopo_url = \"https://www.ngdc.noaa.gov/thredds/dodsC/global/ETOPO2022_15s_surface.nc\"\n",
    "        \n",
    "        try:\n",
    "            # Use xarray to access the remote dataset\n",
    "            etopo_dataset = xr.open_dataset(etopo_url)\n",
    "            \n",
    "            # Filter to our region of interest\n",
    "            etopo_regional = etopo_dataset.sel(\n",
    "                lat=slice(TARGET_REGION['south'], TARGET_REGION['north']),\n",
    "                lon=slice(TARGET_REGION['west'], TARGET_REGION['east'])\n",
    "            )\n",
    "            \n",
    "            bathymetry_data['etopo_bathymetry'] = etopo_regional\n",
    "            print(\"‚úì ETOPO bathymetry data accessed successfully\")\n",
    "            print(f\"  Grid dimensions: {etopo_regional.dims}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"‚úó Error accessing ETOPO data: {e}\")\n",
    "            # Alternative: Use Earth Engine for SRTM data\n",
    "            try:\n",
    "                if 'ee' in globals():\n",
    "                    print(\"Trying SRTM data from Earth Engine...\")\n",
    "                    srtm = ee.Image('USGS/SRTMGL1_003')\n",
    "                    \n",
    "                    # Get elevation for our AOI\n",
    "                    aoi = ee.Geometry.Rectangle([\n",
    "                        TARGET_REGION['west'], TARGET_REGION['south'],\n",
    "                        TARGET_REGION['east'], TARGET_REGION['north']\n",
    "                    ])\n",
    "                    \n",
    "                    srtm_clipped = srtm.clip(aoi)\n",
    "                    bathymetry_data['srtm_elevation'] = srtm_clipped\n",
    "                    print(\"‚úì SRTM elevation data from Earth Engine\")\n",
    "                else:\n",
    "                    print(\"‚ö†Ô∏è Earth Engine not available for SRTM data\")\n",
    "                    bathymetry_data['srtm_elevation'] = None\n",
    "                    \n",
    "            except Exception as ee_error:\n",
    "                print(f\"‚úó Earth Engine SRTM error: {ee_error}\")\n",
    "                bathymetry_data['srtm_elevation'] = None\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"‚úó Error with bathymetry data: {e}\")\n",
    "        bathymetry_data['etopo_bathymetry'] = None\n",
    "    \n",
    "    # GEBCO Download Information\n",
    "    print(\"\\\\nüìã GEBCO Data Download Instructions:\")\n",
    "    print(\"1. Visit: https://download.gebco.net/\")\n",
    "    print(\"2. Select your region coordinates:\")\n",
    "    print(f\"   North: {TARGET_REGION['north']}¬∞\")\n",
    "    print(f\"   South: {TARGET_REGION['south']}¬∞\")\n",
    "    print(f\"   West: {TARGET_REGION['west']}¬∞\")\n",
    "    print(f\"   East: {TARGET_REGION['east']}¬∞\")\n",
    "    print(\"3. Download as NetCDF format\")\n",
    "    print(\"4. Place in ../data/raw/bathymetry/\")\n",
    "    \n",
    "    return bathymetry_data\n",
    "\n",
    "# Fetch bathymetry data\n",
    "bathymetry_terrain_data = ingest_bathymetry_data()\n",
    "\n",
    "print(\"\\\\nBathymetry & Terrain Data Summary:\")\n",
    "for dataset, data in bathymetry_terrain_data.items():\n",
    "    if data is not None:\n",
    "        print(f\"  ‚úì {dataset}: {type(data)}\")\n",
    "        if hasattr(data, 'dims'):\n",
    "            print(f\"    Dimensions: {data.dims}\")\n",
    "        elif hasattr(data, 'getInfo'):\n",
    "            try:\n",
    "                info = data.getInfo()\n",
    "                print(f\"    Earth Engine Image: {info.get('type', 'Unknown')}\")\n",
    "            except:\n",
    "                print(\"    Earth Engine Image (info not accessible)\")\n",
    "    else:\n",
    "        print(f\"  ‚úó {dataset}: Not available\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6263896",
   "metadata": {},
   "source": [
    "## 7. Store Raw Data in Cloud Storage\n",
    "\n",
    "Upload all ingested datasets to cloud storage with proper organization and metadata:\n",
    "\n",
    "- **AWS S3** or **Google Cloud Storage**\n",
    "- Organized by data source and temporal partitions\n",
    "- Include metadata for data lineage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5b1a605",
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_cloud_storage():\n",
    "    \"\"\"\n",
    "    Setup cloud storage for raw data persistence\n",
    "    \"\"\"\n",
    "    storage_config = {}\n",
    "    \n",
    "    # AWS S3 Setup\n",
    "    print(\"Setting up AWS S3 storage...\")\n",
    "    try:\n",
    "        if AWS_ACCESS_KEY != 'your_aws_access_key':\n",
    "            s3_client = boto3.client(\n",
    "                's3',\n",
    "                aws_access_key_id=AWS_ACCESS_KEY,\n",
    "                aws_secret_access_key=AWS_SECRET_KEY\n",
    "            )\n",
    "            \n",
    "            # Define bucket and folder structure\n",
    "            bucket_name = 'mlops-sdg14-marine-data'\n",
    "            folder_structure = {\n",
    "                'satellite': 'raw/satellite_imagery/',\n",
    "                'ocean_physics': 'raw/ocean_physics/',\n",
    "                'insitu_sensors': 'raw/insitu_sensors/',\n",
    "                'species_observations': 'raw/species_observations/',\n",
    "                'bathymetry': 'raw/bathymetry/',\n",
    "                'processed': 'processed/',\n",
    "                'features': 'features/'\n",
    "            }\n",
    "            \n",
    "            storage_config['s3'] = {\n",
    "                'client': s3_client,\n",
    "                'bucket': bucket_name,\n",
    "                'folders': folder_structure\n",
    "            }\n",
    "            print(\"‚úì AWS S3 client configured\")\n",
    "        else:\n",
    "            print(\"‚ö†Ô∏è AWS credentials not provided\")\n",
    "            storage_config['s3'] = None\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"‚úó AWS S3 setup error: {e}\")\n",
    "        storage_config['s3'] = None\n",
    "    \n",
    "    # Google Cloud Storage Setup\n",
    "    print(\"Setting up Google Cloud Storage...\")\n",
    "    try:\n",
    "        # GCS would require service account key\n",
    "        gcs_bucket_name = 'mlops-sdg14-marine-gcs'\n",
    "        print(f\"‚ö†Ô∏è GCS bucket would be: {gcs_bucket_name}\")\n",
    "        print(\"   Requires service account JSON key file\")\n",
    "        storage_config['gcs'] = None\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚úó GCS setup error: {e}\")\n",
    "        storage_config['gcs'] = None\n",
    "    \n",
    "    return storage_config\n",
    "\n",
    "def save_data_locally(all_data):\n",
    "    \"\"\"\n",
    "    Save all ingested data locally with proper organization\n",
    "    \"\"\"\n",
    "    print(\"Saving data locally...\")\n",
    "    \n",
    "    # Create organized directory structure\n",
    "    data_dirs = {\n",
    "        'satellite': raw_data_dir / 'satellite_imagery',\n",
    "        'ocean_physics': raw_data_dir / 'ocean_physics',\n",
    "        'insitu_sensors': raw_data_dir / 'insitu_sensors',\n",
    "        'species_observations': raw_data_dir / 'species_observations',\n",
    "        'bathymetry': raw_data_dir / 'bathymetry'\n",
    "    }\n",
    "    \n",
    "    for name, path in data_dirs.items():\n",
    "        path.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    # Save metadata\n",
    "    metadata = {\n",
    "        'ingestion_date': datetime.now().isoformat(),\n",
    "        'target_region': TARGET_REGION,\n",
    "        'date_range': {'start': START_DATE, 'end': END_DATE},\n",
    "        'data_sources': {}\n",
    "    }\n",
    "    \n",
    "    # Satellite data\n",
    "    if all_data.get('satellite'):\n",
    "        metadata['data_sources']['satellite'] = {\n",
    "            'sources': list(all_data['satellite'].keys()),\n",
    "            'earth_engine_collections': ['NASA/OCEANDATA/MODIS-Aqua/L3SMI']\n",
    "        }\n",
    "    \n",
    "    # Ocean physics data\n",
    "    if all_data.get('ocean_physics'):\n",
    "        sst_data = all_data['ocean_physics'].get('sst_ostia')\n",
    "        if sst_data is not None:\n",
    "            # Save SST data as NetCDF\n",
    "            sst_file = data_dirs['ocean_physics'] / 'sst_ostia_sample.nc'\n",
    "            try:\n",
    "                # Save a small sample\n",
    "                sst_sample = sst_data.isel(time=slice(0, 12))  # First 12 time steps\n",
    "                sst_sample.to_netcdf(sst_file)\n",
    "                print(f\"‚úì Saved SST data: {sst_file}\")\n",
    "                metadata['data_sources']['ocean_physics'] = {\n",
    "                    'sst_file': str(sst_file),\n",
    "                    'dimensions': dict(sst_sample.dims)\n",
    "                }\n",
    "            except Exception as e:\n",
    "                print(f\"‚úó Error saving SST data: {e}\")\n",
    "    \n",
    "    # In-situ sensor data\n",
    "    if all_data.get('insitu'):\n",
    "        # Save Argo data as JSON\n",
    "        argo_data = all_data['insitu'].get('argo_floats')\n",
    "        if argo_data:\n",
    "            argo_file = data_dirs['insitu_sensors'] / 'argo_profiles.json'\n",
    "            with open(argo_file, 'w') as f:\n",
    "                json.dump(argo_data, f, indent=2)\n",
    "            print(f\"‚úì Saved Argo data: {argo_file}\")\n",
    "            metadata['data_sources']['argo_floats'] = {\n",
    "                'file': str(argo_file),\n",
    "                'profiles_count': len(argo_data)\n",
    "            }\n",
    "        \n",
    "        # Save buoy data\n",
    "        buoy_data = all_data['insitu'].get('noaa_buoys')\n",
    "        if buoy_data:\n",
    "            buoy_file = data_dirs['insitu_sensors'] / 'noaa_buoy_data.json'\n",
    "            with open(buoy_file, 'w') as f:\n",
    "                json.dump(buoy_data, f, indent=2)\n",
    "            print(f\"‚úì Saved buoy data: {buoy_file}\")\n",
    "            metadata['data_sources']['noaa_buoys'] = {\n",
    "                'file': str(buoy_file),\n",
    "                'station_id': buoy_data['station_id']\n",
    "            }\n",
    "    \n",
    "    # Species observation data\n",
    "    if all_data.get('species'):\n",
    "        for source, data in all_data['species'].items():\n",
    "            if data:\n",
    "                species_file = data_dirs['species_observations'] / f'{source}.json'\n",
    "                with open(species_file, 'w') as f:\n",
    "                    json.dump(data, f, indent=2)\n",
    "                print(f\"‚úì Saved {source}: {species_file}\")\n",
    "                metadata['data_sources'][source] = {\n",
    "                    'file': str(species_file),\n",
    "                    'observations_count': len(data)\n",
    "                }\n",
    "    \n",
    "    # Save metadata\n",
    "    metadata_file = raw_data_dir / 'ingestion_metadata.json'\n",
    "    with open(metadata_file, 'w') as f:\n",
    "        json.dump(metadata, f, indent=2)\n",
    "    print(f\"‚úì Saved metadata: {metadata_file}\")\n",
    "    \n",
    "    return metadata\n",
    "\n",
    "# Setup storage and save data\n",
    "storage_config = setup_cloud_storage()\n",
    "\n",
    "# Combine all ingested data\n",
    "all_ingested_data = {\n",
    "    'satellite': satellite_data,\n",
    "    'ocean_physics': ocean_physics_data,\n",
    "    'insitu': insitu_data,\n",
    "    'species': species_obs_data,\n",
    "    'bathymetry': bathymetry_terrain_data\n",
    "}\n",
    "\n",
    "# Save data locally\n",
    "ingestion_metadata = save_data_locally(all_ingested_data)\n",
    "\n",
    "print(\"\\\\nüìÅ Local Data Storage Summary:\")\n",
    "print(f\"Raw data directory: {raw_data_dir}\")\n",
    "print(f\"Metadata file: {raw_data_dir / 'ingestion_metadata.json'}\")\n",
    "print(\"\\\\nData organization:\")\n",
    "for category, data in all_ingested_data.items():\n",
    "    if data:\n",
    "        available_sources = [k for k, v in data.items() if v is not None]\n",
    "        print(f\"  üìÇ {category}: {len(available_sources)} sources\")\n",
    "        for source in available_sources:\n",
    "            print(f\"    ‚Ä¢ {source}\")\n",
    "    else:\n",
    "        print(f\"  üìÇ {category}: No data available\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8050dc41",
   "metadata": {},
   "source": [
    "## 8. Data Validation & Next Steps\n",
    "\n",
    "Validate the ingested data and prepare for the next phase of the MLOps pipeline:\n",
    "\n",
    "- **Data Quality Checks**: Completeness, consistency, and format validation\n",
    "- **Data Profiling**: Statistical summaries and distributions\n",
    "- **Visualization**: Quick plots to verify data integrity\n",
    "- **Pipeline Integration**: Prepare for feature engineering phase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "929e8cb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_ingested_data(all_data, metadata):\n",
    "    \"\"\"\n",
    "    Perform comprehensive validation of ingested data\n",
    "    \"\"\"\n",
    "    validation_report = {\n",
    "        'summary': {},\n",
    "        'quality_checks': {},\n",
    "        'recommendations': []\n",
    "    }\n",
    "    \n",
    "    print(\"üîç Validating Ingested Data...\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # Data availability summary\n",
    "    data_summary = {}\n",
    "    for category, datasets in all_data.items():\n",
    "        if datasets:\n",
    "            available = sum(1 for v in datasets.values() if v is not None)\n",
    "            total = len(datasets)\n",
    "            data_summary[category] = f\"{available}/{total} sources available\"\n",
    "        else:\n",
    "            data_summary[category] = \"0/0 sources available\"\n",
    "    \n",
    "    validation_report['summary'] = data_summary\n",
    "    \n",
    "    # Detailed validation\n",
    "    print(\"\\\\nüìä Data Quality Assessment:\")\n",
    "    \n",
    "    # Validate species data\n",
    "    if all_data.get('species'):\n",
    "        species_data = all_data['species']\n",
    "        total_observations = 0\n",
    "        \n",
    "        for source, data in species_data.items():\n",
    "            if data:\n",
    "                count = len(data)\n",
    "                total_observations += count\n",
    "                print(f\"  ‚úì {source}: {count:,} observations\")\n",
    "            else:\n",
    "                print(f\"  ‚úó {source}: No data\")\n",
    "        \n",
    "        validation_report['quality_checks']['species_observations'] = {\n",
    "            'total_observations': total_observations,\n",
    "            'sources_with_data': sum(1 for v in species_data.values() if v is not None)\n",
    "        }\n",
    "        \n",
    "        if total_observations > 0:\n",
    "            validation_report['recommendations'].append(\n",
    "                \"Species data available for biodiversity modeling\"\n",
    "            )\n",
    "        else:\n",
    "            validation_report['recommendations'].append(\n",
    "                \"No species observations - consider alternative data sources\"\n",
    "            )\n",
    "    \n",
    "    # Validate ocean physics data\n",
    "    if all_data.get('ocean_physics'):\n",
    "        ocean_data = all_data['ocean_physics']\n",
    "        sst_data = ocean_data.get('sst_ostia')\n",
    "        \n",
    "        if sst_data is not None:\n",
    "            try:\n",
    "                print(f\"  ‚úì SST data dimensions: {sst_data.dims}\")\n",
    "                print(f\"  ‚úì SST date range: {sst_data.time.min().values} to {sst_data.time.max().values}\")\n",
    "                \n",
    "                validation_report['quality_checks']['sst_data'] = {\n",
    "                    'dimensions': dict(sst_data.dims),\n",
    "                    'time_range': {\n",
    "                        'start': str(sst_data.time.min().values),\n",
    "                        'end': str(sst_data.time.max().values)\n",
    "                    }\n",
    "                }\n",
    "                \n",
    "                validation_report['recommendations'].append(\n",
    "                    \"SST data suitable for temperature forecasting models\"\n",
    "                )\n",
    "            except Exception as e:\n",
    "                print(f\"  ‚úó SST data validation error: {e}\")\n",
    "        else:\n",
    "            print(\"  ‚úó No SST data available\")\n",
    "    \n",
    "    # Validate in-situ sensor data\n",
    "    if all_data.get('insitu'):\n",
    "        insitu_data = all_data['insitu']\n",
    "        \n",
    "        # Argo floats\n",
    "        argo_data = insitu_data.get('argo_floats')\n",
    "        if argo_data:\n",
    "            print(f\"  ‚úì Argo profiles: {len(argo_data)}\")\n",
    "            validation_report['quality_checks']['argo_profiles'] = len(argo_data)\n",
    "        \n",
    "        # Buoy data\n",
    "        buoy_data = insitu_data.get('noaa_buoys')\n",
    "        if buoy_data:\n",
    "            data_records = len(buoy_data.get('data', []))\n",
    "            print(f\"  ‚úì Buoy observations: {data_records}\")\n",
    "            validation_report['quality_checks']['buoy_observations'] = data_records\n",
    "    \n",
    "    # Generate recommendations\n",
    "    print(\"\\\\nüí° Recommendations for Next Steps:\")\n",
    "    \n",
    "    if validation_report['recommendations']:\n",
    "        for i, rec in enumerate(validation_report['recommendations'], 1):\n",
    "            print(f\"  {i}. {rec}\")\n",
    "    \n",
    "    # Additional recommendations based on available data\n",
    "    if total_observations > 100:\n",
    "        print(\"  ‚Ä¢ Sufficient species data for machine learning models\")\n",
    "    else:\n",
    "        print(\"  ‚Ä¢ Consider synthetic data generation for model training\")\n",
    "    \n",
    "    if sst_data is not None:\n",
    "        print(\"  ‚Ä¢ SST data ready for time series forecasting\")\n",
    "    else:\n",
    "        print(\"  ‚Ä¢ Alternative SST sources needed\")\n",
    "    \n",
    "    print(\"  ‚Ä¢ Proceed to feature engineering phase\")\n",
    "    print(\"  ‚Ä¢ Set up Feast feature store for ML pipeline\")\n",
    "    print(\"  ‚Ä¢ Configure Kubeflow pipelines for training orchestration\")\n",
    "    \n",
    "    return validation_report\n",
    "\n",
    "# Quick visualization function\n",
    "def create_data_overview_plots():\n",
    "    \"\"\"\n",
    "    Create quick overview plots of ingested data\n",
    "    \"\"\"\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "    fig.suptitle('Marine Ecosystem Data Ingestion Overview', fontsize=16)\n",
    "    \n",
    "    # Plot 1: Data source availability\n",
    "    categories = list(all_ingested_data.keys())\n",
    "    availability = []\n",
    "    \n",
    "    for category in categories:\n",
    "        if all_ingested_data[category]:\n",
    "            available = sum(1 for v in all_ingested_data[category].values() if v is not None)\n",
    "            total = len(all_ingested_data[category])\n",
    "            availability.append(available / total * 100)\n",
    "        else:\n",
    "            availability.append(0)\n",
    "    \n",
    "    axes[0, 0].bar(categories, availability, color='skyblue', alpha=0.7)\n",
    "    axes[0, 0].set_title('Data Source Availability (%)')\n",
    "    axes[0, 0].set_ylabel('Percentage Available')\n",
    "    axes[0, 0].tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    # Plot 2: Species observations by source\n",
    "    species_counts = []\n",
    "    species_sources = []\n",
    "    \n",
    "    if all_ingested_data.get('species'):\n",
    "        for source, data in all_ingested_data['species'].items():\n",
    "            if data:\n",
    "                species_sources.append(source.replace('_', ' ').title())\n",
    "                species_counts.append(len(data))\n",
    "    \n",
    "    if species_counts:\n",
    "        axes[0, 1].bar(species_sources, species_counts, color='lightgreen', alpha=0.7)\n",
    "        axes[0, 1].set_title('Species Observations by Source')\n",
    "        axes[0, 1].set_ylabel('Number of Observations')\n",
    "        axes[0, 1].tick_params(axis='x', rotation=45)\n",
    "    else:\n",
    "        axes[0, 1].text(0.5, 0.5, 'No Species Data', ha='center', va='center', transform=axes[0, 1].transAxes)\n",
    "        axes[0, 1].set_title('Species Observations by Source')\n",
    "    \n",
    "    # Plot 3: Target region visualization\n",
    "    lon_range = [TARGET_REGION['west'], TARGET_REGION['east']]\n",
    "    lat_range = [TARGET_REGION['south'], TARGET_REGION['north']]\n",
    "    \n",
    "    axes[1, 0].add_patch(plt.Rectangle((lon_range[0], lat_range[0]), \n",
    "                                      lon_range[1] - lon_range[0], \n",
    "                                      lat_range[1] - lat_range[0], \n",
    "                                      fill=False, edgecolor='red', linewidth=2))\n",
    "    axes[1, 0].set_xlim(-180, 180)\n",
    "    axes[1, 0].set_ylim(-90, 90)\n",
    "    axes[1, 0].set_xlabel('Longitude')\n",
    "    axes[1, 0].set_ylabel('Latitude')\n",
    "    axes[1, 0].set_title('Target Region (Pacific Ocean)')\n",
    "    axes[1, 0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Plot 4: Data ingestion timeline\n",
    "    axes[1, 1].text(0.5, 0.5, f'Ingestion Date:\\\\n{datetime.now().strftime(\"%Y-%m-%d %H:%M\")}\\\\n\\\\nDate Range:\\\\n{START_DATE} to {END_DATE}', \n",
    "                   ha='center', va='center', transform=axes[1, 1].transAxes, fontsize=12,\n",
    "                   bbox=dict(boxstyle=\"round,pad=0.3\", facecolor=\"lightblue\", alpha=0.5))\n",
    "    axes[1, 1].set_title('Ingestion Summary')\n",
    "    axes[1, 1].axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Run validation and create overview\n",
    "validation_report = validate_ingested_data(all_ingested_data, ingestion_metadata)\n",
    "create_data_overview_plots()\n",
    "\n",
    "print(\"\\\\n\" + \"=\"*60)\n",
    "print(\"üöÄ DATA INGESTION PIPELINE COMPLETE!\")\n",
    "print(\"=\"*60)\n",
    "print(f\"üìÅ Data saved to: {raw_data_dir}\")\n",
    "print(f\"üìã Metadata: {raw_data_dir / 'ingestion_metadata.json'}\")\n",
    "print(\"\\\\nüîÑ Next Steps:\")\n",
    "print(\"1. Feature Engineering (02_feature_engineering.ipynb)\")\n",
    "print(\"2. Model Training (03_model_training.ipynb)\")\n",
    "print(\"3. Pipeline Orchestration (Kubeflow)\")\n",
    "print(\"4. Model Serving (Seldon Core)\")\n",
    "print(\"5. Monitoring Setup (Evidently + Grafana)\")\n",
    "print(\"\\\\nüåä Ready to build the Marine Ecosystem ML Pipeline!\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
